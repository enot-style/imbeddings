# Required
# Hugging Face access token with read access to model weights.
HF_TOKEN=hf_put_your_token_here

# This variable is only used with Dockerfile.gpu / docker-compose.gpu.yaml
# Select a PyTorch GPU base tag from https://hub.docker.com/r/pytorch/pytorch/tags
# Use the tag only; Dockerfile.gpu resolves to "pytorch/pytorch:TAG".
# Example: 2.10.0-cuda13.0-cudnn9-runtime
PYTORCH_TAG=2.10.0-cuda13.0-cudnn9-runtime

# Device selection: auto (default), cpu, or cuda.
IMBEDDINGS_DEVICE=auto
# Optional CUDA memory cap per process (fraction of total GPU memory).
# Example: 0.5 uses up to 50% of the GPU's memory.
IMBEDDINGS_CUDA_MEMORY_FRACTION=0.5

# Optional: comma/space-separated API keys for Bearer auth on /v1/embeddings.
# Leave empty to disable authentication.
IMBEDDINGS_API_KEYS=

# Max number of model bundles cached in memory.
IMBEDDINGS_MAX_LOADED_MODELS=1

# Service
# Bind host for Uvicorn inside the container/local run.
IMBEDDINGS_HOST=0.0.0.0
# Port for the API (also used for Docker port mapping).
IMBEDDINGS_PORT=8000

# Request limits
# Max images per request batch.
IMBEDDINGS_MAX_BATCH_SIZE=4
# Max decoded image dimensions (pixels).
IMBEDDINGS_MAX_IMAGE_WIDTH=1024
IMBEDDINGS_MAX_IMAGE_HEIGHT=1024
# Max image payload size in bytes.
IMBEDDINGS_MAX_IMAGE_BYTES=102400
# Max image pixels (decompression bomb protection).
IMBEDDINGS_MAX_IMAGE_PIXELS=4000000
# Timeout (seconds) for fetching remote images by URL.
IMBEDDINGS_REMOTE_IMAGE_REQUEST_TIMEOUT=10
# Max redirects when fetching remote images.
IMBEDDINGS_MAX_IMAGE_REDIRECTS=3
# Max request body size in bytes.
IMBEDDINGS_MAX_REQUEST_BYTES=2000000
# Max concurrent inference calls per process.
IMBEDDINGS_MAX_CONCURRENT_INFERENCE=2
# Inference timeout in seconds.
IMBEDDINGS_INFERENCE_TIMEOUT_SECONDS=60
